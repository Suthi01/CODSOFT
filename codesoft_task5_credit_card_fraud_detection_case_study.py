# -*- coding: utf-8 -*-
"""CODESOFT task5 credit card Fraud Detection case study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TB4gh1t8hs3AggCMifG9L9HxRoIy6hN8

**Importing Modules:**
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import pickle

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

from collections import Counter
from imblearn.over_sampling import SMOTE

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier, plot_importance
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, recall_score, classification_report

"""**Importing te dataset:**"""

data = pd.read_csv("/kaggle/input/creditcardfraud/creditcard.csv")
data.head()

data.describe()

"""**Check the Missing Values:**"""

data.isnull().sum()

# Since it is not a Time series problem
data.drop("Time",axis=1,inplace=True)

data.info()

data.shape

data.describe()

"""## Correlation"""

dataplot = sns.heatmap(data.corr(), cmap="YlGnBu")
# display heatmap
plt.show()

"""**Check for class distribution**"""

sns.countplot(x="Class",data=data)

(data["Class"].value_counts()/284807)*100

"""Highly imbalanced dataset with 99% of data as not-fraud and only 0.03% of data as fraud"""

X= data.drop("Class",axis=1)
y=data["Class"]

"""**Oversampling**"""

sm = SMOTE(random_state = 2)
X, y = sm.fit_resample(X, y)
counter = Counter(y)
print(counter)

scale = MinMaxScaler()
X = scale.fit_transform(X)

"""**Model Training**"""

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)
X_train.shape,y_train.shape

"""**Logistic Regression**"""

lr = LogisticRegression()
lr.fit(X_train,y_train)

y_pred_lr = lr.predict(X_test)
print(classification_report(y_test, y_pred_lr))
y_pred_proba_lr = lr.predict_proba(X_test)[::,1]
fpr, tpr, _ = roc_curve(y_test,  y_pred_proba_lr)
auc = roc_auc_score(y_test, y_pred_proba_lr)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

cf_matrix = confusion_matrix(y_test, y_pred_lr)
print(cf_matrix)

pickle.dump(lr, open('lr_model (auc = 0.99).pkl', 'wb'))

sns.heatmap(cf_matrix, annot=True)

"""**Observations**:
Logistic Regression has a high number of False Negatives that can lead to a loss of millions of dollars to the bank. At the same time a significant amount of False positives can affect customers relationship. Logistics regression cannot be considered as a good model.

**XGBoost Classifier**:
"""

xgb = XGBClassifier()
xgb.fit(X_train,y_train)

y_pred = xgb.predict(X_test)
print(classification_report(y_test, y_pred))
y_pred_proba = xgb.predict_proba(X_test)[::,1]
fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)
auc = roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

pickle.dump(xgb, open('xgb_model (auc = 0.99).pkl', 'wb'))

plot_importance(xgb)
plt.show()

cf_matrix = confusion_matrix(y_test, y_pred)
print(cf_matrix)

sns.heatmap(cf_matrix, annot=True)

"""**Observations:**
XGBoost has zero False Negatives hence it is a great model. Also, it predicts small number of False Positives than Logistic Regression.

**Random Forest:**
"""

rf = RandomForestClassifier()
rf.fit(X_train,y_train)

y_pred_rf = rf.predict(X_test)
print(classification_report(y_test, y_pred_rf))
y_pred_proba_rf = rf.predict_proba(X_test)[::,1]
fpr, tpr, _ = roc_curve(y_test,  y_pred_proba_rf)
auc = roc_auc_score(y_test, y_pred_proba_rf)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

cf_matrix = confusion_matrix(y_test, y_pred_rf)
print(cf_matrix)

sns.heatmap(cf_matrix, annot=True)

"""**Observations:**
Random forest has a lowest false positives than xgboost proves to be best model
"""

feature_names = [f"feature {i}" for i in range(X.shape[1])]
importances = rf.feature_importances_
forest_importances = pd.Series(importances, index=feature_names)
std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=std, ax=ax)
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()

pickle.dump(rf, open('rf_model (auc = 0.99 ).pkl', 'wb'))

"""**At Final:**
The models XGBoost and Random Forest have great performance on the test data with 100% recall and 99% Accuracy.
"""